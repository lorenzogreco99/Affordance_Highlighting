{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "c4FdmwP1yl2E"
   },
   "source": [
    "# **General**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "collapsed": true,
    "executionInfo": {
     "elapsed": 63021,
     "status": "ok",
     "timestamp": 1737198100186,
     "user": {
      "displayName": "Lorenzo Greco",
      "userId": "10369371803041585957"
     },
     "user_tz": -60
    },
    "id": "vpE_VLn_jK2j",
    "outputId": "3f5e3701-c8f9-418d-f799-fc5153e4839d"
   },
   "outputs": [],
   "source": [
    "!pip install git+https://github.com/openai/CLIP.git\n",
    "!pip install kaolin==0.17.0 -f https://nvidia-kaolin.s3.us-east-2.amazonaws.com/torch-2.5.1_cu121.html\n",
    "!pip install open3d"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 3598,
     "status": "ok",
     "timestamp": 1736250249202,
     "user": {
      "displayName": "Luciana Colella",
      "userId": "14146049597079620411"
     },
     "user_tz": -60
    },
    "id": "uKwM7eT_jPlw",
    "outputId": "e17b1b45-5da4-45be-ebb5-3fe7fafd4fe9"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Mounted at /content/drive\n"
     ]
    }
   ],
   "source": [
    "from google.colab import drive\n",
    "drive.mount('/content/drive', force_remount=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 226,
     "status": "ok",
     "timestamp": 1736250266094,
     "user": {
      "displayName": "Luciana Colella",
      "userId": "14146049597079620411"
     },
     "user_tz": -60
    },
    "id": "2EGsM2eLlAUo",
    "outputId": "89982e73-2454-47bc-e154-7d9dccdafd75"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/content/drive/MyDrive/AML Project: 3D Affordance/Affordance_Highlighting_Project_2024-main\n"
     ]
    }
   ],
   "source": [
    "%cd /content/drive/MyDrive/AML Project: 3D Affordance/Affordance_Highlighting_Project_2024-main"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "ejhrQ5VSyqTn"
   },
   "source": [
    "# **Load dataset**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 40858,
     "status": "ok",
     "timestamp": 1736250311619,
     "user": {
      "displayName": "Luciana Colella",
      "userId": "14146049597079620411"
     },
     "user_tz": -60
    },
    "id": "_fHsZ6MKW9mh",
    "outputId": "af7ed5db-c63b-4afe-c22e-fe5c960f1a26"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Type: <class 'list'>\n"
     ]
    }
   ],
   "source": [
    "from dataset import load_data\n",
    "\n",
    "data = load_data('/content/drive/MyDrive/AML Project: 3D Affordance/Affordance_Highlighting_Project_2024-main/full_shape_train_data.pkl')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "kBGrGtiAyueR"
   },
   "source": [
    "# **Model Functions**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "JdD_7ANwyht-"
   },
   "outputs": [],
   "source": [
    "\n",
    "import clip\n",
    "import copy\n",
    "import json\n",
    "import kaolin as kal\n",
    "import kaolin.ops.mesh\n",
    "import numpy as np\n",
    "import os\n",
    "import random\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torchvision\n",
    "\n",
    "from itertools import permutations, product\n",
    "from Normalization import MeshNormalizer\n",
    "from mesh import Mesh\n",
    "from pathlib import Path\n",
    "from render import Renderer\n",
    "from tqdm import tqdm\n",
    "from torch.autograd import grad\n",
    "from torchvision import transforms\n",
    "from utils import device, color_mesh\n",
    "\n",
    "# Neural Highlighter model\n",
    "class NeuralHighlighter(nn.Module):\n",
    "    def __init__(self, depth, width, out_dim, input_dim=3):\n",
    "        super(NeuralHighlighter, self).__init__()\n",
    "\n",
    "        layers = []\n",
    "\n",
    "        layers.append(nn.Linear(input_dim, width))\n",
    "        layers.append(nn.ReLU())\n",
    "        layers.append(nn.LayerNorm([width]))\n",
    "\n",
    "        for _ in range(depth):\n",
    "            layers.append(nn.Linear(width, width))\n",
    "            layers.append(nn.ReLU())\n",
    "            layers.append(nn.LayerNorm([width]))\n",
    "\n",
    "        layers.append(nn.Linear(width, out_dim))\n",
    "        layers.append(nn.Softmax(dim=1))\n",
    "\n",
    "        self.mlp = nn.ModuleList(layers)\n",
    "\n",
    "    def forward(self, x):\n",
    "        for layer in self.mlp:\n",
    "            x = layer(x)\n",
    "        return x\n",
    "\n",
    "def get_clip_model(clipmodel):\n",
    "    #Load CLIP model\n",
    "    clip_model, preprocess = clip.load(clipmodel, device)\n",
    "    return clip_model, preprocess\n",
    "\n",
    "# ================== HELPER FUNCTIONS =============================\n",
    "def save_final_results(log_dir, name, mesh, mlp, vertices, colors, render, background):\n",
    "    mlp.eval()\n",
    "    with torch.no_grad():\n",
    "        probs = mlp(vertices)\n",
    "        max_idx = torch.argmax(probs, 1, keepdim=True)\n",
    "        # for renders\n",
    "        one_hot = torch.zeros(probs.shape).to(device)\n",
    "        one_hot = one_hot.scatter_(1, max_idx, 1)\n",
    "        sampled_mesh = mesh\n",
    "\n",
    "        highlight = torch.tensor([204, 255, 0]).to(device)\n",
    "        gray = torch.tensor([180, 180, 180]).to(device)\n",
    "        colors = torch.stack((highlight/255, gray/255)).to(device)\n",
    "        color_mesh(one_hot, sampled_mesh, colors)\n",
    "        rendered_images, _, _ = render.render_views(sampled_mesh, num_views=5,\n",
    "                                                                        show=False,\n",
    "                                                                        center_azim=0,\n",
    "                                                                        center_elev=0,\n",
    "                                                                        std=4,\n",
    "                                                                        return_views=True,\n",
    "                                                                        lighting=True,\n",
    "                                                                        background=background)\n",
    "        # for mesh\n",
    "        final_color = torch.zeros(vertices.shape[0], 3).to(device)\n",
    "        final_color = torch.where(max_idx==0, highlight, gray)\n",
    "        mesh.export(os.path.join(log_dir, f\"{name}.ply\"), extension=\"ply\", color=final_color)\n",
    "        save_renders(log_dir, 0, rendered_images, name='final_render.jpg')\n",
    "\n",
    "\n",
    "def clip_loss(n_augs, rendered_images, encoded_text, clip_transform, augment_transform, clip_model):\n",
    "      loss = 0.0\n",
    "      for _ in range(n_augs):\n",
    "          augmented_image = augment_transform(rendered_images)\n",
    "          encoded_renders = clip_model.encode_image(augmented_image)\n",
    "\n",
    "          if encoded_text.shape[0] > 1:\n",
    "              loss -= torch.cosine_similarity(torch.mean(encoded_renders, dim=0),\n",
    "                                                torch.mean(encoded_text, dim=0), dim=0)\n",
    "          else:\n",
    "              loss -= torch.cosine_similarity(torch.mean(encoded_renders, dim=0, keepdim=True),\n",
    "                                                encoded_text)\n",
    "\n",
    "      return 1 + (loss/n_augs)\n",
    "\n",
    "\n",
    "def save_renders(dir, i, rendered_images, name=None):\n",
    "    if name is not None:\n",
    "        torchvision.utils.save_image(rendered_images, os.path.join(dir, name))\n",
    "    else:\n",
    "        torchvision.utils.save_image(rendered_images, os.path.join(dir, 'renders/iter_{}.jpg'.format(i)))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "m3d4WZlqy3gZ"
   },
   "source": [
    "# **Mesh from point cloud**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "IpQ9vRQOzf7d"
   },
   "outputs": [],
   "source": [
    "import plotly.graph_objects as go\n",
    "import open3d as o3d\n",
    "from scipy.spatial import cKDTree\n",
    "from render import Renderer\n",
    "from utils import device, color_mesh"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "nUT1ZIulzH7K"
   },
   "source": [
    "**Point cloud to mesh function**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "CUce3qy4Wicl"
   },
   "outputs": [],
   "source": [
    "def pc_to_mesh(data, selected_obj_index):\n",
    "    shape_id = data[selected_obj_index]['shape_id']\n",
    "    semantic_class = data[selected_obj_index]['semantic class']\n",
    "    affordances = data[selected_obj_index]['affordance']\n",
    "    coordinates = data[selected_obj_index]['full_shape']['coordinate']\n",
    "    labels = data[selected_obj_index]['full_shape']['label']\n",
    "    #Save coordinate file\n",
    "    np.savetxt(\"coordinates.txt\", coordinates, fmt=\"%.6f\", comments=\"\")\n",
    "\n",
    "    #Point Cloud without normals\n",
    "    pcd = o3d.io.read_point_cloud(\"coordinates.txt\", format='xyz')\n",
    "\n",
    "    # Adding normals to the point cloud\n",
    "    pcd.estimate_normals(search_param=o3d.geometry.KDTreeSearchParamHybrid(radius=0.1, max_nn=20))\n",
    "    pcd.orient_normals_consistent_tangent_plane(k=20)\n",
    "\n",
    "    #Creating mesh with Poisson surface reconstruction\n",
    "    mesh_from_pc, densities = o3d.geometry.TriangleMesh.create_from_point_cloud_poisson(pcd, depth=9)\n",
    "    return mesh_from_pc, coordinates,  labels\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "l0ve9LgpzPWd"
   },
   "source": [
    "**Visualizing function**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "CIKvH8GCCYz4"
   },
   "outputs": [],
   "source": [
    "def show_mesh(mesh, coordinates, th, labels = None):\n",
    "  # Colors\n",
    "  base_color = 'rgb(169, 169, 169)' #grey\n",
    "  highlithed_color = 'rgb(255, 255, 0)' #yellow\n",
    "\n",
    "  # Get info mesh\n",
    "  vertices = np.asarray(mesh.vertices)\n",
    "  faces = np.asarray(mesh.triangles)\n",
    "\n",
    "  if labels is None:\n",
    "    print(\"Mesh without Highlighting\")\n",
    "    fig = go.Figure(\n",
    "            data=[\n",
    "                go.Mesh3d(\n",
    "                    x=vertices[:, 0], y=vertices[:, 1], z=vertices[:, 2],\n",
    "                    i=faces[:, 0], j=faces[:, 1], k=faces[:, 2],\n",
    "                    color=base_color,\n",
    "                    opacity=1\n",
    "                  )\n",
    "            ])\n",
    "\n",
    "  else:\n",
    "    print(\"Mesh with Highlighting\")\n",
    "    # Handling coloring mesh\n",
    "    kdtree = cKDTree(coordinates)\n",
    "    _, idx = kdtree.query(vertices)\n",
    "    vertex_colors = labels[idx]\n",
    "\n",
    "    vertex_intensity = np.where(vertex_colors > th, vertex_colors, 0)\n",
    "\n",
    "    fig = go.Figure(\n",
    "        data=[\n",
    "            go.Mesh3d(\n",
    "                x=vertices[:, 0], y=vertices[:, 1], z=vertices[:, 2],\n",
    "                i=faces[:, 0], j=faces[:, 1], k=faces[:, 2],\n",
    "                intensity=vertex_intensity,\n",
    "                colorscale = [\n",
    "                  [0, base_color],  # Grey (Low density)\n",
    "                  [1, highlithed_color]     # Yellow (High density)\n",
    "                ],  \n",
    "                colorbar=dict(title='Density'),\n",
    "                opacity=1\n",
    "        )])\n",
    "\n",
    "  fig.update_layout(\n",
    "      title= 'Highlithed Mesh' if labels is not None else '3D Mesh from Point Cloud',\n",
    "      scene=dict(\n",
    "          xaxis=dict(showgrid=False, showticklabels=False, showbackground=False, title=''),\n",
    "          yaxis=dict(showgrid=False, showticklabels=False, showbackground=False, title=''),\n",
    "          zaxis=dict(showgrid=False, showticklabels=False, showbackground=False, title=''),\n",
    "          aspectmode='data'\n",
    "      ),\n",
    "  )\n",
    "\n",
    "  fig.show()\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "FuqDetS0zstI"
   },
   "source": [
    "# **Evaluation pipeline**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "XMnG21lhz2nP"
   },
   "source": [
    "**Choose settings**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "OvAgwAAcz8MT"
   },
   "outputs": [],
   "source": [
    "eval_indexes = [7368, 7370, 7371, 7374, 7375, 7376]\n",
    "\n",
    "# Choose specific object\n",
    "index = eval_indexes[3]\n",
    "# Choose affordance class\n",
    "affordance_class = 'wrap_grasp'\n",
    "\n",
    "# Choose threshold for density\n",
    "th = 0.2\n",
    "\n",
    "# HYPERPARAMETERS\n",
    "# Choose hyper-parameters\n",
    "LR = 0.0001\n",
    "augs = 7\n",
    "views = 5\n",
    "dep = 4\n",
    "\n",
    "# Choose augmentation transforms\n",
    "extra_augs = False #True if you want additional augmentation transforms, False if you want std augmentation transforms\n",
    "\n",
    "# Choose Learing Rate schedule\n",
    "add_sheduler = False #True if you want a stepLR schedule\n",
    "# Scheduler settings\n",
    "step_size = 1000\n",
    "gamma = 0.1\n",
    "\n",
    "\n",
    "# Output path\n",
    "output_dir_custom = f'./output_val/bottle_{index}/output_LR00001'\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "v2bdRHmO1vsN"
   },
   "source": [
    "### GT Highlithed Mesh"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "4TP3dTyHF7Uv"
   },
   "outputs": [],
   "source": [
    "# Creating mesh\n",
    "mesh0, pcd, labels = pc_to_mesh(data, index)\n",
    "# Saving mesh\n",
    "o3d.io.write_triangle_mesh(f\"mesh_{index}.obj\", mesh0)\n",
    "# visualizing mesh\n",
    "show_mesh(mesh0, pcd, th, labels[affordance_class].flatten())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Taa7yzfv13Ry"
   },
   "source": [
    "### Our model Highlithed Mesh"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "CGZtl-A5Ehc5"
   },
   "outputs": [],
   "source": [
    "# Constrain most sources of randomness\n",
    "# (some torch backwards functions within CLIP are non-determinstic)\n",
    "seed = 0\n",
    "torch.manual_seed(seed)\n",
    "torch.cuda.manual_seed(seed)\n",
    "torch.cuda.manual_seed_all(seed)\n",
    "random.seed(seed)\n",
    "np.random.seed(seed)\n",
    "torch.backends.cudnn.benchmark = False\n",
    "torch.backends.cudnn.deterministic = True\n",
    "\n",
    "\n",
    "render_res = 224\n",
    "learning_rate = LR\n",
    "n_iter = 2500\n",
    "res = 224\n",
    "\n",
    "obj_path = f'mesh_{index}.obj'\n",
    "\n",
    "n_augs = augs\n",
    "output_dir = output_dir_custom\n",
    "clip_model = 'ViT-B/32'\n",
    "\n",
    "Path(os.path.join(output_dir, 'renders')).mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "objbase, extension = os.path.splitext(os.path.basename(obj_path))\n",
    "\n",
    "render = Renderer(dim=(render_res, render_res))\n",
    "mesh = Mesh(obj_path)\n",
    "MeshNormalizer(mesh)()\n",
    "\n",
    "# Initialize variables\n",
    "background = torch.tensor((1., 1., 1.)).to(device)\n",
    "\n",
    "log_dir = output_dir\n",
    "\n",
    "#parametri NeuralHighlighter\n",
    "depth = dep\n",
    "width = 256\n",
    "n_classes=2\n",
    "input_dim=3\n",
    "positional_encoding=True\n",
    "sigma=5.0\n",
    "\n",
    "# CLIP and Augmentation Transforms\n",
    "clip_normalizer = transforms.Normalize((0.48145466, 0.4578275, 0.40821073), (0.26862954, 0.26130258, 0.27577711))\n",
    "clip_transform = transforms.Compose([\n",
    "    transforms.Resize((res, res)),\n",
    "    clip_normalizer\n",
    "])\n",
    "\n",
    "if extra_augs == False:\n",
    "          augment_transform = transforms.Compose([\n",
    "              transforms.RandomResizedCrop(res, scale=(1, 1)),\n",
    "              transforms.RandomPerspective(fill=1, p=0.8, distortion_scale=0.5),\n",
    "              clip_normalizer\n",
    "          ])\n",
    "else:\n",
    "          augment_transform = transforms.Compose([\n",
    "              transforms.RandomResizedCrop(res, scale=(0.8, 1.0)), \n",
    "              transforms.RandomPerspective(fill=1, p=0.5, distortion_scale=0.3), \n",
    "              transforms.RandomHorizontalFlip(p=0.5), \n",
    "              transforms.RandomRotation(degrees=15), \n",
    "              transforms.ColorJitter(brightness=0.1, contrast=0.1, saturation=0.1, hue=0.05),\n",
    "              transforms.RandomAffine(degrees=10, translate=(0.05, 0.05), scale=(0.9, 1.1)), \n",
    "\n",
    "              clip_normalizer\n",
    "          ])\n",
    "\n",
    "\n",
    "# MLP Settings\n",
    "mlp = NeuralHighlighter(depth, width, n_classes, input_dim, positional_encoding, sigma).to(device)\n",
    "optim = torch.optim.Adam(mlp.parameters(), learning_rate)\n",
    "\n",
    "if add_sheduler:\n",
    "    scheduler = torch.optim.lr_scheduler.StepLR(optim, step_size=step_size, gamma=gamma)\n",
    "\n",
    "# list of possible colors\n",
    "rgb_to_color = {(204/255, 1., 0.): \"highlighter\", (180/255, 180/255, 180/255): \"gray\"}\n",
    "color_to_rgb = {\"highlighter\": [204/255, 1., 0.], \"gray\": [180/255, 180/255, 180/255]}\n",
    "full_colors = [[204/255, 1., 0.], [180/255, 180/255, 180/255]]\n",
    "colors = torch.tensor(full_colors).to(device)\n",
    "\n",
    "\n",
    "# --- Prompt ---\n",
    "# encode prompt with CLIP\n",
    "clip_model, preprocess = get_clip_model(clip_model)\n",
    "\n",
    "# encode prompt with CLIP\n",
    "prompt = \"A 3D render of a gray {} with the {} area highlighted\".format('bottle', 'grasped and wrapped')\n",
    "with torch.no_grad():\n",
    "    prompt_token = clip.tokenize([prompt]).to(device)\n",
    "    encoded_text = clip_model.encode_text(prompt_token)\n",
    "    encoded_text = encoded_text / encoded_text.norm(dim=1, keepdim=True)\n",
    "\n",
    "\n",
    "vertices = copy.deepcopy(mesh.vertices)\n",
    "n_views = views\n",
    "\n",
    "losses = []\n",
    "\n",
    "# Optimization loop\n",
    "for i in tqdm(range(n_iter)):\n",
    "    optim.zero_grad()\n",
    "\n",
    "    # predict highlight probabilities\n",
    "    pred_class = mlp(vertices)\n",
    "    # color and render mesh\n",
    "    sampled_mesh = mesh\n",
    "    color_mesh(pred_class, sampled_mesh, colors)\n",
    "    rendered_images, elev, azim = render.render_views(sampled_mesh, num_views=n_views,\n",
    "                                                            show=False,\n",
    "                                                            center_azim=0,\n",
    "                                                            center_elev=0,\n",
    "                                                            std=4,\n",
    "                                                            return_views=True,\n",
    "                                                            lighting=True,\n",
    "                                                            background=background)\n",
    "\n",
    "    # Calculate CLIP Loss\n",
    "    loss = clip_loss(n_augs, rendered_images, encoded_text, clip_transform, augment_transform, clip_model)\n",
    "    loss.backward(retain_graph=True)\n",
    "\n",
    "    optim.step()\n",
    "\n",
    "    if add_sheduler:\n",
    "          scheduler.step()\n",
    "\n",
    "    # update variables + record loss\n",
    "    with torch.no_grad():\n",
    "        losses.append(loss.item())\n",
    "\n",
    "    # report results\n",
    "    if i % 100 == 0:\n",
    "        print(\"Last 100 CLIP score: {}\".format(np.mean(losses[-100:])))\n",
    "        save_renders(log_dir, i, rendered_images)\n",
    "        with open(os.path.join(log_dir, \"training_info.txt\"), \"a\") as f:\n",
    "            f.write(f\"For iteration {i}... Prompt: {prompt}, Last 100 avg CLIP score: {np.mean(losses[-100:])}, CLIP score {losses[-1]}\\n\")\n",
    "\n",
    "name='prova'\n",
    "# save results\n",
    "save_final_results(log_dir, name, mesh, mlp, vertices, colors, render, background)\n",
    "\n",
    "# Save prompts\n",
    "with open(os.path.join(output_dir, prompt), \"w\") as f:\n",
    "    f.write('')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "EdzbD5oc2Dak"
   },
   "source": [
    "# Calculate metric mIOU"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "XhM8nGoR2P0b"
   },
   "outputs": [],
   "source": [
    "def compute_iou_single_class(pred, gt):\n",
    "    intersection = np.sum((pred == 1) & (gt == 1))\n",
    "    union = np.sum((pred == 1) | (gt == 1))\n",
    "    if union == 0:\n",
    "        return 1.0\n",
    "    else:\n",
    "        iou = intersection / union\n",
    "        return iou"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "D41_EWE843wA"
   },
   "outputs": [],
   "source": [
    "#Handling the two different domains\n",
    "dominance = (pred_class[:, 0] > pred_class[:, 1]).long()\n",
    "kdtree = cKDTree(pcd)\n",
    "_, idx = kdtree.query(vertices.cpu())\n",
    "vertex_colors = labels[affordance_class].flatten()[idx]\n",
    "\n",
    "pred=dominance.cpu().numpy()\n",
    "print(pred)\n",
    "gt = (vertex_colors > th).astype(int)\n",
    "print(gt)\n",
    "\n",
    "#computing iou\n",
    "iou = compute_iou_single_class(pred, gt)\n",
    "print(\"\")\n",
    "print(\"IoU:\", iou)\n"
   ]
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "gpuType": "T4",
   "provenance": [
    {
     "file_id": "10Z86uLJpM8UBgJ1S1NHZGWHVmjr63iCO",
     "timestamp": 1736253742050
    },
    {
     "file_id": "1mOYCiiijbKt8l5wHS4pHq4kCJt6pJ7dW",
     "timestamp": 1736249247287
    }
   ]
  },
  "kernelspec": {
   "display_name": "Python 3",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
