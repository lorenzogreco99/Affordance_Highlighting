{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "c4FdmwP1yl2E"
   },
   "source": [
    "# **General**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "collapsed": true,
    "executionInfo": {
     "elapsed": 68330,
     "status": "ok",
     "timestamp": 1736935574747,
     "user": {
      "displayName": "lorenzo greco",
      "userId": "03090194177258534449"
     },
     "user_tz": -60
    },
    "id": "vpE_VLn_jK2j",
    "outputId": "aa18bae9-37a4-4035-c5f3-fa8f66281341"
   },
   "outputs": [],
   "source": [
    "!pip install git+https://github.com/openai/CLIP.git\n",
    "!pip install kaolin==0.17.0 -f https://nvidia-kaolin.s3.us-east-2.amazonaws.com/torch-2.5.1_cu121.html\n",
    "!pip install open3d"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 29887,
     "status": "ok",
     "timestamp": 1736864330699,
     "user": {
      "displayName": "Lorenzo Greco",
      "userId": "10369371803041585957"
     },
     "user_tz": -60
    },
    "id": "uKwM7eT_jPlw",
    "outputId": "cef31263-2f38-4e90-c8b2-545498fa9ca4"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Mounted at /content/drive\n"
     ]
    }
   ],
   "source": [
    "from google.colab import drive\n",
    "drive.mount('/content/drive', force_remount=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 604,
     "status": "ok",
     "timestamp": 1736864331296,
     "user": {
      "displayName": "Lorenzo Greco",
      "userId": "10369371803041585957"
     },
     "user_tz": -60
    },
    "id": "2EGsM2eLlAUo",
    "outputId": "581dc117-1613-405b-c8a8-e11700349c0a"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/content/drive/.shortcut-targets-by-id/1YTt044XOkry2sxZchKykdntCTnmMraQo/AML Project: 3D Affordance/Affordance_Highlighting_Project_2024-main\n"
     ]
    }
   ],
   "source": [
    "%cd /content/drive/MyDrive/AML Project: 3D Affordance/Affordance_Highlighting_Project_2024-main"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "kBGrGtiAyueR"
   },
   "source": [
    "# **Model Functions**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "JdD_7ANwyht-"
   },
   "outputs": [],
   "source": [
    "\n",
    "import clip\n",
    "import copy\n",
    "import json\n",
    "import kaolin as kal\n",
    "import kaolin.ops.mesh\n",
    "import numpy as np\n",
    "import os\n",
    "import random\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torchvision\n",
    "\n",
    "from itertools import permutations, product\n",
    "from Normalization import MeshNormalizer\n",
    "from mesh import Mesh\n",
    "from pathlib import Path\n",
    "from render import Renderer\n",
    "from tqdm import tqdm\n",
    "from torch.autograd import grad\n",
    "from torchvision import transforms\n",
    "from utils import device, color_mesh\n",
    "\n",
    "# Neural Highlighter model\n",
    "class NeuralHighlighter(nn.Module):\n",
    "    def __init__(self, depth, width, out_dim, input_dim=3):\n",
    "        super(NeuralHighlighter, self).__init__()\n",
    "\n",
    "        layers = []\n",
    "\n",
    "        layers.append(nn.Linear(input_dim, width))\n",
    "        layers.append(nn.ReLU())\n",
    "        layers.append(nn.LayerNorm([width]))\n",
    "\n",
    "        for _ in range(depth):\n",
    "            layers.append(nn.Linear(width, width))\n",
    "            layers.append(nn.ReLU())\n",
    "            layers.append(nn.LayerNorm([width]))\n",
    "\n",
    "        layers.append(nn.Linear(width, out_dim))\n",
    "        layers.append(nn.Softmax(dim=1))\n",
    "\n",
    "        self.mlp = nn.ModuleList(layers)\n",
    "\n",
    "    def forward(self, x):\n",
    "        for layer in self.mlp:\n",
    "            x = layer(x)\n",
    "        return x\n",
    "\n",
    "def get_clip_model(clipmodel):\n",
    "    #Load CLIP model\n",
    "    clip_model, preprocess = clip.load(clipmodel, device)\n",
    "    return clip_model, preprocess\n",
    "\n",
    "# ================== HELPER FUNCTIONS =============================\n",
    "def save_final_results(log_dir, name, mesh, mlp, vertices, colors, render, background):\n",
    "    mlp.eval()\n",
    "    with torch.no_grad():\n",
    "        probs = mlp(vertices)\n",
    "        max_idx = torch.argmax(probs, 1, keepdim=True)\n",
    "        # for renders\n",
    "        one_hot = torch.zeros(probs.shape).to(device)\n",
    "        one_hot = one_hot.scatter_(1, max_idx, 1)\n",
    "        sampled_mesh = mesh\n",
    "\n",
    "        highlight = torch.tensor([204, 255, 0]).to(device)\n",
    "        gray = torch.tensor([180, 180, 180]).to(device)\n",
    "        colors = torch.stack((highlight/255, gray/255)).to(device)\n",
    "        color_mesh(one_hot, sampled_mesh, colors)\n",
    "        rendered_images, _, _ = render.render_views(sampled_mesh, num_views=5,\n",
    "                                                                        show=False,\n",
    "                                                                        center_azim=0,\n",
    "                                                                        center_elev=0,\n",
    "                                                                        std=4,\n",
    "                                                                        return_views=True,\n",
    "                                                                        lighting=True,\n",
    "                                                                        background=background)\n",
    "        # for mesh\n",
    "        final_color = torch.zeros(vertices.shape[0], 3).to(device)\n",
    "        final_color = torch.where(max_idx==0, highlight, gray)\n",
    "        mesh.export(os.path.join(log_dir, f\"{name}.ply\"), extension=\"ply\", color=final_color)\n",
    "        save_renders(log_dir, 0, rendered_images, name='final_render.jpg')\n",
    "\n",
    "\n",
    "\n",
    "def clip_loss(n_augs, rendered_images, encoded_text, clip_transform, augment_transform, clip_model):\n",
    "      loss = 0.0\n",
    "      for _ in range(n_augs):\n",
    "          augmented_image = augment_transform(rendered_images)\n",
    "          encoded_renders = clip_model.encode_image(augmented_image)\n",
    "\n",
    "          if encoded_text.shape[0] > 1:\n",
    "              loss -= torch.cosine_similarity(torch.mean(encoded_renders, dim=0),\n",
    "                                                torch.mean(encoded_text, dim=0), dim=0)\n",
    "          else:\n",
    "              loss -= torch.cosine_similarity(torch.mean(encoded_renders, dim=0, keepdim=True),\n",
    "                                                encoded_text)\n",
    "\n",
    "      return 1 + (loss/n_augs)\n",
    "\n",
    "\n",
    "def save_renders(dir, i, rendered_images, name=None):\n",
    "    if name is not None:\n",
    "        torchvision.utils.save_image(rendered_images, os.path.join(dir, name))\n",
    "    else:\n",
    "        torchvision.utils.save_image(rendered_images, os.path.join(dir, 'renders/iter_{}.jpg'.format(i)))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "OvAgwAAcz8MT"
   },
   "outputs": [],
   "source": [
    "# Choose affordance class\n",
    "affordance_class = 'wrap_grasp'\n",
    "\n",
    "# HYPERPARAMETERS\n",
    "# Choose hyper-parameters\n",
    "LR = 0.0001\n",
    "augs = 7\n",
    "views = 5\n",
    "dep = 4\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Taa7yzfv13Ry"
   },
   "source": [
    "### Our model Highlithed Mesh"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 1264362,
     "status": "ok",
     "timestamp": 1736874967474,
     "user": {
      "displayName": "Lorenzo Greco",
      "userId": "10369371803041585957"
     },
     "user_tz": -60
    },
    "id": "CGZtl-A5Ehc5",
    "outputId": "acb16480-0497-4868-f899-c02802fad121"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:kaolin.rep.surface_mesh:Attribute \"face_normals\" has not been set and failed to be computed due to: index -1 is out of bounds for dimension 1 with size 0\n",
      "WARNING:kaolin.rep.surface_mesh:Unexpected type passed to requires_grad None\n",
      "WARNING:kaolin.rep.surface_mesh:Attribute \"vertex_normals\" has not been set and failed to be computed due to: 'NoneType' object has no attribute 'unsqueeze'\n",
      "WARNING:kaolin.rep.surface_mesh:Attribute \"face_normals\" has not been set and failed to be computed due to: index -1 is out of bounds for dimension 1 with size 0\n",
      "  0%|          | 1/2500 [00:00<25:03,  1.66it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Last 100 CLIP score: 0.70849609375\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  4%|▍         | 101/2500 [00:50<19:20,  2.07it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Last 100 CLIP score: 0.668642578125\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  8%|▊         | 201/2500 [01:40<21:49,  1.76it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Last 100 CLIP score: 0.6596044921875\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 12%|█▏        | 301/2500 [02:29<17:35,  2.08it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Last 100 CLIP score: 0.654482421875\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 16%|█▌        | 401/2500 [03:20<17:23,  2.01it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Last 100 CLIP score: 0.65201171875\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 20%|██        | 501/2500 [04:10<20:32,  1.62it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Last 100 CLIP score: 0.6510107421875\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 24%|██▍       | 601/2500 [05:00<14:59,  2.11it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Last 100 CLIP score: 0.64943359375\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 28%|██▊       | 701/2500 [05:50<14:44,  2.03it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Last 100 CLIP score: 0.648671875\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 32%|███▏      | 801/2500 [06:41<15:20,  1.85it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Last 100 CLIP score: 0.6524072265625\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 36%|███▌      | 901/2500 [07:31<13:42,  1.94it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Last 100 CLIP score: 0.647802734375\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 40%|████      | 1001/2500 [08:21<12:21,  2.02it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Last 100 CLIP score: 0.6480322265625\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 44%|████▍     | 1101/2500 [09:11<12:09,  1.92it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Last 100 CLIP score: 0.6478955078125\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 48%|████▊     | 1201/2500 [10:01<10:21,  2.09it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Last 100 CLIP score: 0.64623046875\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 52%|█████▏    | 1301/2500 [10:52<10:14,  1.95it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Last 100 CLIP score: 0.6502783203125\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 56%|█████▌    | 1401/2500 [11:41<09:01,  2.03it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Last 100 CLIP score: 0.646865234375\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 60%|██████    | 1501/2500 [12:31<08:03,  2.07it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Last 100 CLIP score: 0.64966796875\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 64%|██████▍   | 1601/2500 [13:21<07:46,  1.93it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Last 100 CLIP score: 0.6496337890625\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 68%|██████▊   | 1701/2500 [14:10<06:22,  2.09it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Last 100 CLIP score: 0.6466943359375\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 72%|███████▏  | 1801/2500 [15:02<05:39,  2.06it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Last 100 CLIP score: 0.6494384765625\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 76%|███████▌  | 1901/2500 [15:51<05:29,  1.82it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Last 100 CLIP score: 0.647490234375\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 80%|████████  | 2001/2500 [16:40<03:58,  2.10it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Last 100 CLIP score: 0.649814453125\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 84%|████████▍ | 2101/2500 [17:30<03:14,  2.05it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Last 100 CLIP score: 0.6502587890625\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 88%|████████▊ | 2201/2500 [18:20<02:40,  1.86it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Last 100 CLIP score: 0.64708984375\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 92%|█████████▏| 2301/2500 [19:09<01:36,  2.07it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Last 100 CLIP score: 0.646044921875\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 96%|█████████▌| 2401/2500 [19:59<00:49,  2.02it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Last 100 CLIP score: 0.648681640625\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 2500/2500 [20:48<00:00,  2.00it/s]\n"
     ]
    }
   ],
   "source": [
    "# Constrain most sources of randomness\n",
    "# (some torch backwards functions within CLIP are non-determinstic)\n",
    "seed = 0\n",
    "torch.manual_seed(seed)\n",
    "torch.cuda.manual_seed(seed)\n",
    "torch.cuda.manual_seed_all(seed)\n",
    "random.seed(seed)\n",
    "np.random.seed(seed)\n",
    "torch.backends.cudnn.benchmark = False\n",
    "torch.backends.cudnn.deterministic = True\n",
    "\n",
    "\n",
    "render_res = 224\n",
    "learning_rate = LR\n",
    "n_iter = 2500\n",
    "res = 224\n",
    "obj='jagermeister'\n",
    "obj_path = f'/content/drive/MyDrive/AML Project: 3D Affordance/mesh_create/{obj}.obj'\n",
    "# Output path\n",
    "output_dir_custom = f'./output_estenzione/{obj}_openable'\n",
    "n_augs = augs\n",
    "output_dir = output_dir_custom\n",
    "clip_model = 'ViT-B/32'\n",
    "\n",
    "Path(os.path.join(output_dir, 'renders')).mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "objbase, extension = os.path.splitext(os.path.basename(obj_path))\n",
    "\n",
    "render = Renderer(dim=(render_res, render_res))\n",
    "mesh = Mesh(obj_path)\n",
    "MeshNormalizer(mesh)()\n",
    "\n",
    "# Initialize variables\n",
    "background = torch.tensor((1., 1., 1.)).to(device)\n",
    "\n",
    "log_dir = output_dir\n",
    "\n",
    "#parametri NeuralHighlighter\n",
    "depth = dep\n",
    "width = 256\n",
    "n_classes=2\n",
    "input_dim=3\n",
    "positional_encoding=True\n",
    "sigma=5.0\n",
    "\n",
    "# CLIP and Augmentation Transforms\n",
    "clip_normalizer = transforms.Normalize((0.48145466, 0.4578275, 0.40821073), (0.26862954, 0.26130258, 0.27577711))\n",
    "clip_transform = transforms.Compose([\n",
    "    transforms.Resize((res, res)),\n",
    "    clip_normalizer\n",
    "])\n",
    "augment_transform = transforms.Compose([\n",
    "              transforms.RandomResizedCrop(res, scale=(1, 1)),\n",
    "              transforms.RandomPerspective(fill=1, p=0.8, distortion_scale=0.5),\n",
    "              clip_normalizer\n",
    "          ])\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "# MLP Settings\n",
    "mlp = NeuralHighlighter(depth, width, n_classes, input_dim, positional_encoding, sigma).to(device)\n",
    "optim = torch.optim.Adam(mlp.parameters(), learning_rate)\n",
    "\n",
    "\n",
    "# list of possible colors\n",
    "rgb_to_color = {(204/255, 1., 0.): \"highlighter\", (180/255, 180/255, 180/255): \"gray\"}\n",
    "color_to_rgb = {\"highlighter\": [204/255, 1., 0.], \"gray\": [180/255, 180/255, 180/255]}\n",
    "full_colors = [[204/255, 1., 0.], [180/255, 180/255, 180/255]]\n",
    "colors = torch.tensor(full_colors).to(device)\n",
    "\n",
    "\n",
    "# --- Prompt ---\n",
    "# encode prompt with CLIP\n",
    "clip_model, preprocess = get_clip_model(clip_model)\n",
    "\n",
    "# encode prompt with CLIP\n",
    "prompt = \"A 3D render of a gray {} with the {} area highlighted\".format('bottle', 'openable')\n",
    "with torch.no_grad():\n",
    "    prompt_token = clip.tokenize([prompt]).to(device)\n",
    "    encoded_text = clip_model.encode_text(prompt_token)\n",
    "    encoded_text = encoded_text / encoded_text.norm(dim=1, keepdim=True)\n",
    "\n",
    "\n",
    "vertices = copy.deepcopy(mesh.vertices)\n",
    "n_views = views\n",
    "\n",
    "losses = []\n",
    "\n",
    "# Optimization loop\n",
    "for i in tqdm(range(n_iter)):\n",
    "    optim.zero_grad()\n",
    "\n",
    "    # predict highlight probabilities\n",
    "    pred_class = mlp(vertices)\n",
    "    # color and render mesh\n",
    "    sampled_mesh = mesh\n",
    "    color_mesh(pred_class, sampled_mesh, colors)\n",
    "    rendered_images, elev, azim = render.render_views(sampled_mesh, num_views=n_views,\n",
    "                                                            show=False,\n",
    "                                                            center_azim=0,\n",
    "                                                            center_elev=0,\n",
    "                                                            std=4,\n",
    "                                                            return_views=True,\n",
    "                                                            lighting=True,\n",
    "                                                            background=background)\n",
    "\n",
    "    # Calculate CLIP Loss\n",
    "    loss = clip_loss(n_augs, rendered_images, encoded_text, clip_transform, augment_transform, clip_model)\n",
    "    loss.backward(retain_graph=True)\n",
    "\n",
    "    optim.step()\n",
    "\n",
    "    # update variables + record loss\n",
    "    with torch.no_grad():\n",
    "        losses.append(loss.item())\n",
    "\n",
    "    # report results\n",
    "    if i % 100 == 0:\n",
    "        print(\"Last 100 CLIP score: {}\".format(np.mean(losses[-100:])))\n",
    "        save_renders(log_dir, i, rendered_images)\n",
    "        with open(os.path.join(log_dir, \"training_info.txt\"), \"a\") as f:\n",
    "            f.write(f\"For iteration {i}... Prompt: {prompt}, Last 100 avg CLIP score: {np.mean(losses[-100:])}, CLIP score {losses[-1]}\\n\")\n",
    "\n",
    "name='prova'\n",
    "# save results\n",
    "save_final_results(log_dir, name, mesh, mlp, vertices, colors, render, background)\n",
    "\n",
    "# Save prompts\n",
    "with open(os.path.join(output_dir, prompt), \"w\") as f:\n",
    "    f.write('')"
   ]
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "gpuType": "T4",
   "provenance": [
    {
     "file_id": "10Z86uLJpM8UBgJ1S1NHZGWHVmjr63iCO",
     "timestamp": 1736253742050
    },
    {
     "file_id": "1mOYCiiijbKt8l5wHS4pHq4kCJt6pJ7dW",
     "timestamp": 1736249247287
    }
   ]
  },
  "kernelspec": {
   "display_name": "Python 3",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
